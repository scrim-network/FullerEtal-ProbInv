read
    McInnerny & Keller 2008, jump probability, proposal matrix
        this applies to uniform priors on parameters not uniform likelihood on data given model

        wide enough priors should prevent this bias from being a problem?
            the problem is things fall off the cliff near the edges, so probably

        is there a proposal matrix I should be using?
            is there code associated with the paper?

    Roe & Baker - climate sensitivity, science
        gaussian prior in parameter
            Urban and Keller 2009 write about how this is stupid
                is this the Tellus paper?

    Tellus paper.  find joint distribution of parameters

    Oppenheimer paper

    RDM
        snakes?
            Garner, Reed, Keller 2016, last figure, DICE snake

Klaus orders
    1 page summary:  title, 1 sentence summary, 3 key points, authors, nat/sci paragraph, sketch figures
        points
            1. Pfeffer - physical limit - how fast can ice flow - 2 bounds - uncertainty
            2. Paleo record - Tony needs prior - helps inform prior
            3. expert assessment - likelihood funciton - physically motivated priors

    use rejection sampling to post process posterior
        Tierney 1994 is the reference

        we have the log likelihood chain
            can produce the density from this using exp(llik)
                but the density is the same for all samples

        better to use bkde
            need SLR in 2100 to bin the data.  need to record as part of assimilation.
                obj function might need to return both y value and density?
                    instead, have likelihood record y and fix it up like fixing up llik?

                use exception handling to catch and reset assimctx$save_y flag

                save normalized sea level in 2100

            approxfun will do linear interpolation with the return of bkde
                x <- log(rgamma(150,5))
                df <- approxfun(density(x))
                plot(density(x))
                xnew <- c(0.45,1.84,2.3)
                points(xnew,df(xnew),col=2)

    3 prior plot with 90% contour (10% contour)
        dot at highest density

    6 panel PDF/CDF for comparing exp assess only to all data

    figures
        source('makefig.R'); figAisPriors(); figCmpPriors(); figPredict(); figUber(); figCmpInst()

code
    add chain diagnostic code from Kelsey and Tony
        Gelman & Rubin stats
            below 1.1 or 1.05, then converged

            as a function of iterations
                    500K converged

            for two chains

    Latin hypercube
        where are NaNs
            what does it say about physical relationships?

    noise realization
        look at paper and code

        bootstrap the residuals?
            how do we do this with an MCMC chain?

    look at constraints
        try DeConto and Pollard
            look at their paper and Kelsey's treatment
                see Slack convo

    start from LIG?

    look at LHS and priors from Tony assimilation code
        Tony assim does not run.  missing Tg, anto.a, and anto.b

    more predictions
        autocorrelation -- LAG -- minimum LAG -- ACF -- superimpose
            when flattened, not enough predictions?

compare code
    rsync -n --stats -hh -v -H -x -a --delete-after --exclude Scratch/ woju.scrim.psu.edu:/woju/s0/klr324/Ruckertetal_DAIS_codes/ ~/cRucker/
    diff -br --exclude \*pdf --exclude \*csv --exclude \*jpeg --exclude \*mat --exclude \*RData --exclude \*tif ruckert_dais ~/cRucker

    rsync -n --stats -hh -v -H -x -a --delete-after --exclude Random_out/ --exclude Workspace --exclude DAIS_matlab/OtimizedInitialParameters.csv --exclude DAIS_matlab/old_DataCode_versions/DAIS_Matlab_MCMCcalibration.mat --exclude old_code_attempts/dais_error.txt --exclude .git/ woju.scrim.psu.edu:/woju/s0/klr324/Ruckertetal_DAIS_codes ~/Ruckertetal_DAIS_codes.ORIG
