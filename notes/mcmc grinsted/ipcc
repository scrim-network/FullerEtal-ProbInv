smoothing

    file <- "miub_echo_g.txt"
    control <- loadIpccSeaLevel(paste("~/research/slr/ipcc/control/",  file, sep=""), ar4bias=F)$annual
    plot(control)
    smooth <- lowess(control, f=0.25)
    control[, 2] <- smooth$y
    lines(control)

        file <- "miub_echo_g.txt"
        ctl  <- loadIpccTemp(paste("~/research/slr/ipcc/control_gmst/", file, sep=""), ar4bias=F)$annual
        plot(ctl)
        smooth <- lowess(ctl, f=0.75)
        ctl[,2] <- smooth$y
        lines(ctl)

first try -- mri_cgcm2_3_2a
    make SSE work again
        b/c tau=3000 is a bad starting pt
            no, sse fit has same problem
                altho, now a=0.5

    grinRunAssim(nbatch=100000,initial=T); grinRunAssim (nbatch= 100000,mult=0.03)
        mult=0.03 is okay for first proposal

    need to look at tas control runs
        looks like scattershot
            about 0.1 degree downward slope in control run over 350 years

    has more substantial ar(2) component
        grinConfigAssim (ar=2,ipcc= T)

    seeks out high tau:
        grinConfigAssim(ar=2, ipcc=T)
        grinRunAssim(nbatch=100000, initial=T)
        grinRunAssim(nbatch=100000, mult=0.03)
        grinRunAssim(mult=0.03)
        grinRunAssim(mult=0.03)
        grinRunPredict(calibration=T)

    force low tau:
        grinConfigAssim(ar=2,ipcc=T,fp=c(tau=300))

        totally misses in hindcast/projection?

cutnpaste
    source('predict.R')
    prqplot(prctx=prgrinctx)
    #lines(grinassimctx$obs, lwd=2)

    source('pdfplot.R')
    pdfplot(assimctx=grinassimctx)

what to use for s0?

second try -- miub_echo_g
    seeking out high tau
        in both sse and likelihood

    grinConfigAssim (ipcc=T)
    grinRunAssim(nbatch=100000, initial=T)
    grinRunAssim(nbatch=100000, mult=0.03)  # accept is 21.5%
    grinRunAssim(mult=0.03)  # accept is 22%
    grinRunPredict(calibration=T)

third try (AR1:)
    grinConfigAssim(ipcc=T)
    grinRunAssim(nbatch=1000000,initial=T)
    grinRunPredict(calibration=T)

fourth try (AR2:)
    grinConfigAssim(ipcc=T,ar=2)
    grinRunAssim(nbatch=100000,initial=T)
    [1] "using initial scale:"
          s0        a        b      tau     rho1     rho2    sigma 
     0.00320  0.09500  0.05000 29.50000  0.00198  0.00198  0.00010 

       user  system elapsed 
     48.262   0.004  48.350 
    [1] "accept=0.15933"


    # accept is only 0.14
    # need rho limit?

    # EXAMPLE WHERE INITIAL CONDITIONS MATTER!
    grinConfigAssim(ipcc=T,ar=2,useSSE=T)
    grinRunAssim(nbatch=100000,initial=T)
    [1] "using initial scale:"
          s0        a        b      tau     rho1     rho2    sigma 
     0.00320  0.09500  0.05000 29.50000  0.00198  0.00198  0.00010 

       user  system elapsed 
     46.358   0.000  46.400 
    [1] "accept=0.01059"


fifth try (AR2, brute force:)
    grinConfigAssim(ipcc=T,ar=2)
    grinRunAssim(nbatch=5000000,initial=T)
    grinRunPredict(calibration=T)


If you want to be extra careful, you could just fix rho1 and rho2 at
the values estimated by pacf() instead of letting them be uncertain,
and see what that does.  You could fix the innovation sigma1 and
sigma2 also.  arima() will tell you all those best estimates if you
ask it to fit an AR(2) model explicitly.

    fixing rhos made acceptance rate worse!

    step size for sigma is too large?
        changed ubound_sigma to 0.1 instead of 1
            helped accept rate immensely


load("~/runs/paper/pred_ipcc_ar2_run1")
source("predict.R")
prqplot(prctx=prgrinctx,xmax=2000,shade=F,caption="GCM Assimilation Hindcast AR(2)",outfiles=F)

# make sure the line is commented in predict.R
prqplot(prctx=prgrinctx,xmin=2001,shade=F,caption="GCM Assimilation Forecast AR(2)",outfiles=F)
